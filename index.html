<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Xueyang Yu (俞薛钖)</title>

    <meta name="author" content="Xueyang Yu">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/photo.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:72%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Xueyang Yu 
                </p>
                <p>I am a first year Master student at <a href="=https://www.umass.edu/">University of Massachusetts Amherst</a>, working with <a href="https://people.csail.mit.edu/ganchuang/">Prof. Chuang Gan</a>.
                </p>
                <p>
                  Previously, I received my bachelor's degree in Computer Science from <a href="https://www.shanghaitech.edu.cn/eng/">ShanghaiTech University</a> under the supervision of <a href="https://faculty.sist.shanghaitech.edu.cn/faculty/hexm/index.html">Prof. Xuming He</a>. I was also fortunate to be mentored by <a href="https://yxw.cs.illinois.edu/">Prof. Yu-xiong Wang</a> at <a href="https://illinois.edu/">UIUC</a> and <a href="https://yossigandelsman.github.io/">Dr. Yossi Gandelsman</a> at <a href="https://bair.berkeley.edu/">BAIR</a> during my undergraduate.
                </p>
                <p>
                  <p><font color="red"> I am actively seeking for 26/27 fall PhD opportunities in Multiodal Models and Embodied AI. Feel free to chat with me if you are interested!</font>
                </p>
                <p style="text-align:center">
                  <a href="mailto:xueyangyu02@gmail.com">Email</a> &nbsp;/&nbsp;
                  <!-- <a href="data/cvs/CV-2024-12.pdf">CV</a> &nbsp;/&nbsp; -->
                  
                  <a href="https://scholar.google.com/citations?user=AIm87GIAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://x.com/Xueyang_Y">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/Unicorn53547">Github</a> &nbsp;/&nbsp;
                  <a href="images/profile/wechat.jpg">WeChat</a>
                </p>
              </td>
              <td style="padding:2.5%;width:28%;max-width:28%">
                <a href="images/profile/photo.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/profile/photo.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>News</h2>
                <ul>
                  <li>2025.10: <a href="https://vlm-mirage.github.io/">Mirage</a> received <em><font color="red">Best Paper award</font></em>  in <a href="https://knowledgemr-workshop.github.io/">ICCV 2025 KnowledgeMR</a>.</li>
                  <li>2025.09: One paper <a href="https://talkcuts.github.io/">TalkCuts</a> accepted by <a href="https://neurips.cc/Conferences/2025">NeurIPS 2025</a>.</li>
                  <li>2025.09: One paper <a href="https://unicorn53547.github.io/video_syn_rep/">Synthetic Video Representations</a> accepted by <a href="https://iccv2025-limit-workshop.limitlab.xyz/">ICCV 2025 LIMIT</a>.</li>
                  <li>2025.06: One paper <a href="https://www.arxiv.org/abs/2412.10471">VCA</a> accepted by <a href="https://iccv.thecvf.com/Conferences/2025">ICCV 2025</a>.</li>
                  <li>2025.06: Introduce <a href="https://vlm-mirage.github.io/">Mirage</a>.</li>
                  <li>2025.02: One paper <a href="https://arxiv.org/abs/2504.07962">GLUS</a> accepted by <a href="https://cvpr.thecvf.com/Conferences/2025">CVPR 2025</a>.</li>
                  <li>2025.02: Starting my master at UMass Amherst.</li>
                </ul>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I am broadly interested in <b>multi-modal foundation models</b> and <b>spatial intelligence</b>. Currently, I focus on <b>exploring effective multimodal reasoning chains</b> and <b>building models that can reason about dynamics in 2D/3D environments</b>. Reach out if you want to discuss research projects, collaborations or anything else!
                <br>
                <br>
                (<sup>‡</sup>: Equal contribution)
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr> </tr>


      <tr>
        <td style="padding:16px;width:25%;vertical-align:middle">
          <img src="images/talkcuts.png" alt="clean-usnob" width="300" height="150">
        </td>
        <td style="padding:8px;width:75%;vertical-align:middle">
          <a href="https://talkcuts.github.io/">
            <span class="papertitle">TalkCuts: A Large-Scale Dataset for Multi-Shot Human Speech Video Generation</span>
          </a>
          <br>
          <a href="https://jiabenchen.github.io/">Jiaben Chen<sup>‡</sup></a>,
          Zixin Wang<sup>‡</sup>,
          <a href="https://ailingzeng.site/">Ailing Zeng</a>,
          <a href="https://www.yangfu.site/">Yang Fu</a>,
          <strong>Xueyang Yu</strong> ,
          <a href="https://sy77777en.github.io/">Siyuan Cen</a>,
          <a href="https://scholar.google.com/citations?user=eVHCoTsAAAAJ&hl=en">Julian Tanke</a>,
          Yihang Chen,
          <a href="https://scholar.google.com/citations?user=UT-g5BAAAAAJ&hl=en">Koichi Saito</a>,
          <a href="https://www.yukimitsufuji.com/">Yuki Mitsufuji</a>,
          and <a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a>
          <br>
          <em>NeurIPS 2025</em>
          <br>
          <a href="https://talkcuts.github.io/">Project</a> 
          |
          <a href="https://arxiv.org/abs/2510.07249">Paper</a> 
          |
          <a href="https://github.com/UMass-Embodied-AGI/TalkCuts">Code</a>
          <p></p>
          <p>We present TalkCuts, a large-scale benchmark dataset designed to facilitate the study of multi-shot human speech video generation.</p>
        </td>
      </tr>

      <tr>
        <td style="padding:16px;width:25%;vertical-align:middle">
          <video width="300" height="150" autoplay loop muted playsinline>
            <source src="images/mirage.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </td>
        <td style="padding:8px;width:75%;vertical-align:middle">
          <a href="https://vlm-mirage.github.io//">
        <span class="papertitle">Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual Tokens</span>
          </a>
          <br>
            <a href="https://miicheyang.github.io/">Zeyuan Yang<sup>‡</sup></a>,
            <strong>Xueyang Yu</strong><sup>‡</sup>,
            <a href="https://chendl02.github.io/">Delin Chen</a>,
            <a href="https://maohaos2.github.io/Maohao/">Maohao Shen</a>,
            <a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a>
          <br>
          <em>In submission</em>, 2025
          <br
          <em>ICCV 2025, KnowledgeMR workshop (<font color="red">Best Paper</font>) </em>
          <br>
          <a href="https://vlm-mirage.github.io/">Project</a>
          |
          <a href="https://www.arxiv.org/abs/2506.17218">Paper</a>
          |
          <a href="https://github.com/UMass-Embodied-AGI/Mirage">Code</a>
          <p></p>
          <p>
          We propose Mirage, interleaving latent visual tokens, which represent compact imagery visual features, with explicit text tokens to solve diverse multimodal reasoning tasks, boosting the reasoning performance without the full pixel-level image generation.
          </p>
        </td>
      </tr>

      <tr>
        <td style="padding:16px;width:25%;vertical-align:middle">
          <img src="images/VCA.jpg" alt="clean-usnob" width="300" height="150">
        </td>
        <td style="padding:8px;width:75%;vertical-align:middle">
          <a href="https://vis-www.cs.umass.edu/vca-website/">
        <span class="papertitle">VCA: Video Curious Agent for Long Video Understanding</span>
          </a>
          <br>
            <a href="https://miicheyang.github.io/">Zeyuan Yang<sup>‡</sup></a>,
            <a href="https://chendl02.github.io/">Delin Chen<sup>‡</sup></a>,
            <strong>Xueyang Yu</strong>,
            <a href="https://maohaos2.github.io/Maohao/">Maohao Shen</a>,
            <a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a>
          <br>
          <em>ICCV 2025</em>
          <br>
          <a href="https://vis-www.cs.umass.edu/vca-website/">Project</a>
          |
          <a href="https://www.arxiv.org/abs/2412.10471">Paper</a>
          <p></p>
          <p>
            We introduce VCA, a curiosity-driven video agent with self-exploration capability, which autonomously navigates video segments and efficiently builds a comprehensive understanding of complex video sequences.
          </p>
        </td>
      </tr>


      <tr>
        <td style="padding:16px;width:25%;vertical-align:middle">
          <img src="images/GLUS.png" alt="clean-usnob" width="300" height="150">
        </td>
        <td style="padding:8px;width:75%;vertical-align:middle">
          <a href="https://glus-video.github.io/">
        <span class="papertitle">GLUS: Global-Local Reasoning Unified into A Single Large Language Model for Video Segmentation</span>
          </a>
          <br>
            <a href="https://swindblowing.github.io/homepage/">Lang Lin<sup>‡</sup></a>,
            <strong>Xueyang Yu</strong><sup>‡</sup>,
            <a href="https://ziqipang.github.io/">Ziqi Pang<sup>‡</sup></a>,
            <a href="https://yxw.cs.illinois.edu/">Yu-xiong Wang</a>
          <br>
          <em>CVPR 2025</em>
          <br>
          <a href="https://glus-video.github.io/">Project</a>
          |
          <a href="https://arxiv.org/abs/2504.07962">Paper</a>
          |
          <a href="https://github.com/GLUS-video/GLUS">Code</a>
          <p></p>
          <p>
            We propose a simple yet effective MLLMs for language-instructed video segmentation. It emphasizes global-local video understanding and achieves SOTA performance on multiple benchmarks.
          </p>
        </td>
      </tr>


      <tr>
        <td style="padding:16px;width:25%;vertical-align:middle">
          <img src="images/synthetic_video_rep.png" alt="clean-usnob" width="300" height="150">
        </td>
        <td style="padding:8px;width:75%;vertical-align:middle">
          <a href="https://unicorn53547.github.io/video_syn_rep/">
        <span class="papertitle">Learning Video Representations
          without Natural Videos</span>
          </a>
          <br>
            <strong>Xueyang Yu</strong>,
            <a href="https://xinleic.xyz/">Xinlei Chen</a>,
            <a href="https://yossigandelsman.github.io/">Yossi Gandelsman</a>
          <br>
          <em>ICCV 2025, LIMIT workshop</em>
          <br>
          <a href="https://unicorn53547.github.io/video_syn_rep/">Project</a>
          |
          <a href="https://arxiv.org/abs/2410.24213">Paper</a>
          |
          <a href="https://github.com/Unicorn53547/Synthetic-Video-Representations">Code</a>
          <p></p>
          <p>
            We show that synthetic videos and natural images can replace real videos for pre-training, achieving comparable or better performance while offering a controllable, transparent alternative.
          </p>
        </td>
      </tr>

      

      <!-- <tr onmouseout="vca_stop()" onmouseover="vca_start()">
        <td style="padding:16px;width:20%;vertical-align:middle">
          <div class="one">
            <div class="two" id='cat4d_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/vca.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/cat4d.jpg' width="160">
          </div>
          <script type="text/javascript">
            function vca_start() {
              document.getElementById('cat4d_image').style.opacity = "1";
            }

            function vca_stop() {
              document.getElementById('cat4d_image').style.opacity = "0";
            }
            cat4d_stop()
          </script>
        </td>
        <td style="padding:8px;width:80%;vertical-align:middle">
          <a href="https://vis-www.cs.umass.edu/vca-website/">
        <span class="papertitle">VCA: Video Curious Agent for Long Video Understanding</span>
          </a>
          <br>
            <a href="https://miicheyang.github.io/">Zeyuan Yang<sup>‡</sup>,
            <a href="https://chendl02.github.io/">Delin Chen<sup>‡</sup></a>,
            <a href="https://openreview.net/profile?id=~Xueyang_Yu1">Xueyang Yu</a>,
            <a href="https://maohaos2.github.io/Maohao/">Maohao Shen</a>,
            <a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a>
          <br>
          <em>arXiv</em>, 2024
          <br>
          <a href="https://vis-www.cs.umass.edu/vca-website/">project page</a>
          /
          <a href="https://www.arxiv.org/abs/2412.10471">arXiv</a>
          <p></p>
          <p>
            In this work, we introduce VCA, a curiosity-driven video agent with self-exploration capability, which autonomously navigates video segments and efficiently builds a comprehensive understanding of complex video sequences.
          </p>
        </td>
      </tr> -->

      

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
        <tr>
          <td style="padding:0px">
            <h2><b>Service</b></h2>
          </td>
        </tr>
      </tbody></table>

      <!-- <table width="100%" align="center" border="0" cellpadding="20"><tbody>  
        <tr>
          <td width="100%" valign="center" style="padding:0px">
            <ul>
              <li>
                <em>Conference Reviewer:</em> 
                <br>
                ARR 2024, ICML 2025
              </li>
              <li>
                <em>Teaching Assistant:</em>
                <br>
                <span class="papertitle">Objected Oriented Programming, University of Massachusetts, Amherst <em>(Fall 2024)</em> </span>
                <br>
                <span class="papertitle">Foundations of Programming, University of Massachusetts, Amherst <em>(Spring 2025)</em> </span>
            </ul>
          </td>
        </tr>      
      </tbody></table> -->

      <table width="100%" align="center" border="0" cellpadding="20"><tbody>
        <tr>
          <td width="100%" valign="center" style="padding:0px">
            <ul>
              <li>
                <em><strong>Conference Reviewer:</strong></em><br>
                ICLR 2025, ICLR 2026
              </li>
    
              <!-- <li style="margin-top:1em;">
                <em><strong>Teaching Assistant:</strong></em>
                <ul style="list-style: none; margin:0; padding:0; margin-top:0.5em;">
                  
                </ul>
              </li> -->
            </ul>
          </td>
        </tr>
      </tbody></table>


      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
        <br>
        <tr>
          <td style="padding:0px; text-align: center;">
            🎬 🥃 ✈️ 🏀 🍳 
            <!-- <span style="font-size: 10px">💖</span> -->
          </td>
        </tr>
      </tbody></table>
            
        </td>
      </tr>
    </table>
  </body>
</html>
